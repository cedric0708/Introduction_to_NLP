# CS224N Lecture 3: 신경망 학습 – 수작업 및 알고리즘을 통한 그래디언트 계산
## 📌 전체 요약
- 이 강의는 신경망의 학습 과정, 특히 그래디언트 계산을 수학적으로 어떻게 수행하는지 설명하고, 그것을 알고리즘적으로 구현하는 방식(즉, 역전파)까지 다룸.

- 주된 목적:
~~~
- 행렬 미분(matrix calculus)을 통해 그래디언트를 손으로 계산하는 법

- 그리고 역전파(backpropagation) 알고리즘을 통해 자동으로 그래디언트를 계산하는 법 이해하기
~~~
## 🧠 핵심 내용 요약
### 1. 뉴럴 네트워크 계산 구조
- 입력 벡터는 단어 임베딩을 포함

- 각 뉴런의 활성화 함수로는 ReLU, tanh, sigmoid, GELU 등이 쓰임

- 비선형 함수는 모델이 선형 이상의 복잡한 함수를 학습할 수 있게 해줌

### 2. 손실 함수: Cross Entropy Loss
- 분류 문제에서 정답 클래스의 확률을 최대화하는 목표

- 크로스 엔트로피는 정답 확률분포(p)와 모델의 예측 확률분포(q) 간 차이를 수치화

### 3. 그래디언트 계산
- 수학적 관점: 미분의 정의를 사용해 변수 간 변화율(기울기) 계산

- 벡터 입력 → 그레디언트는 편미분 벡터

- 다변량 함수 → 야코비안(Jacobian) 행렬을 사용

### 4. 체인 룰(Chain Rule)
- 함수가 여러 개의 중첩으로 이뤄졌을 때, 그래디언트를 단계별로 곱해서 계산

- 다층 신경망에서 매우 중요!

### 5. 역전파 (Backpropagation)
- 계산 그래프를 따라 forward pass (순전파)로 중간값 저장

- 이후 뒤로 따라가면서 그래디언트를 계산

- 각각의 노드에서:
~~~
- 입력 → 출력의 로컬 그래디언트 계산

- 업스트림 그래디언트와 곱해 → 다운스트림 전달
~~~
### 6. 계산 최적화
- 중복된 계산 방지 위해 이미 계산한 그래디언트 재사용

- 하나의 스칼라 출력에 대해 모든 파라미터에 대한 그래디언트를 한 번에 계산 가능

### 7. 프레임워크와 오토그래드
- PyTorch, TensorFlow 등은 내부적으로 위 계산을 자동화

- 하지만, 직접 구현 원리를 알아야 디버깅, 커스텀 레이어 구현 시 도움이 됨

## 기타내용
- GELU와 같은 비선형 함수는 트랜스포머 모델(BERT 등)에서 자주 사용됨

- 학습률(learning rate) α는 업데이트의 크기를 결정함

- Jacobian vs Shape Convention:
~~~
수학적으로 Jacobian을 따르되, 구현에서는 shape convention (파라미터 형태)을 따름
~~~

