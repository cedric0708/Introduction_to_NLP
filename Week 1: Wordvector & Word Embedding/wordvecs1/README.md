# 📚 강의 개요
## 강의 목표

- NLP에 딥러닝을 어떻게 적용하는지 기본부터 최신 방법까지 학습

- 언어의 본질과 컴퓨터가 언어를 이해하기 어려운 이유 이해

- PyTorch를 사용해 주요 NLP 문제 해결 시스템 구축

# 주요 주제

- 언어와 단어 의미

- Word2Vec 소개

- Word2Vec의 목표 함수와 그래디언트

- 최적화 기초 (Gradient Descent)

# 🧠 단어 의미를 표현하는 방법
## 전통적인 방법
### WordNet 같은 지식 기반 사용
~~~
단점: 의미의 미묘한 차이 반영 어려움, 업데이트 어려움, 주관적
~~~

### One-hot 벡터
~~~
단어를 고차원에서 하나의 1과 나머지 0으로 표현

문제: 유사도를 계산할 수 없음 (벡터들이 직교함)
~~~
# 💡 해결책: 분산 표현 (Distributed Representation)
## Distributional Semantics
~~~
"단어는 함께 자주 등장하는 단어로 의미가 드러난다"

단어 주변의 문맥을 바탕으로 단어 의미를 학습

단어들을 고차원 실수 벡터로 표현 (Word Embedding)
~~~
# 🧩 Word2Vec 소개
## 핵심 아이디어
~~~
대규모 텍스트 코퍼스에서 각 단어를 벡터로 표현

중심 단어(c)가 주어졌을 때, 주변 단어(o)를 예측

이 확률을 최대화하도록 벡터를 학습
~~~
## 목표 함수

- 주어진 중심 단어로 주변 단어를 맞히는 확률의 로그를 최대화 (또는 손실 최소화)

## Softmax 사용

- 중심 단어와 주변 단어 벡터 간 내적(dot product)을 확률로 변환

# 🧮 최적화: Gradient Descent
## 목표: 손실 함수 J(θ)를 최소화

### 방법
~~~
파라미터의 그래디언트를 계산해서 손실을 줄이는 방향으로 업데이트

SGD (Stochastic Gradient Descent)

전체 데이터 대신 일부 샘플만 사용해 빠르게 학습
~~~
# 🔍 요점 요약
- 단어 의미는 고차원 실수 벡터로 잘 표현 가능함

- Word2Vec은 문맥 기반 학습으로 유사한 의미의 단어를 비슷한 벡터로 표현

- 확률적 예측을 통해 단어 임베딩을 최적화

- Gradient Descent로 벡터를 학습하며, SGD를 통해 학습 속도 개선


